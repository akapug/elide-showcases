/**
 * Proximal Policy Optimization (PPO) Implementation
 *
 * Advanced PPO algorithm with:
 * - Clipped surrogate objective
 * - Actor-Critic architecture
 * - Generalized Advantage Estimation (GAE)
 * - Multiple epochs per update
 * - Entropy regularization
 * - Value function clipping
 *
 * Demonstrates Elide's polyglot capabilities with PyTorch and NumPy
 * seamlessly integrated into TypeScript.
 */

// @ts-ignore - PyTorch for neural networks
import torch from 'python:torch';
// @ts-ignore - NumPy for numerical operations
import numpy from 'python:numpy';
// @ts-ignore - Python collections for data structures
import collections from 'python:collections';

// ============================================================================
// Type Definitions
// ============================================================================

export interface PPOConfig {
  stateShape: number[];
  actionSize: number;
  continuous?: boolean; // Continuous or discrete action space
  learningRateActor?: number;
  learningRateCritic?: number;
  discount?: number; // gamma
  gaeÎ»?: number; // GAE lambda
  clipRatio?: number; // PPO clip parameter
  valueClip?: number; // Value function clip parameter
  entropyCoef?: number; // Entropy coefficient
  valueCoef?: number; // Value loss coefficient
  maxGradNorm?: number; // Gradient clipping
  updateEpochs?: number; // Epochs per update
  batchSize?: number;
  trajectorySize?: number; // Steps before update
  device?: string;
}

export interface PPOMemory {
  states: any[];
  actions: any[];
  logProbs: any[];
  rewards: any[];
  values: any[];
  dones: any[];
}

export interface PPOMetrics {
  policyLoss: number;
  valueLoss: number;
  entropy: number;
  totalLoss: number;
  kl: number;
  clipFraction: number;
  explainedVariance: number;
}

export interface TrajectoryBatch {
  states: any;
  actions: any;
  oldLogProbs: any;
  advantages: any;
  returns: any;
  values: any;
}

// ============================================================================
// Actor Network (Policy)
// ============================================================================

export class ActorNetwork {
  private network: any;
  private actionSize: number;
  private continuous: boolean;
  private device: any;

  constructor(
    stateShape: number[],
    actionSize: number,
    continuous = false,
    device = 'cpu'
  ) {
    this.actionSize = actionSize;
    this.continuous = continuous;
    this.device = torch.device(device);

    const inputSize = stateShape.reduce((a, b) => a * b, 1);

    // Build policy network
    this.network = torch.nn.Sequential(
      torch.nn.Linear(inputSize, 256),
      torch.nn.ReLU(),
      torch.nn.Linear(256, 256),
      torch.nn.ReLU(),
      torch.nn.Linear(256, 128),
      torch.nn.ReLU()
    );

    // Output layers depend on action space type
    if (continuous) {
      // For continuous actions, output mean and log_std
      this.network.add_module('mean', torch.nn.Linear(128, actionSize));
      this.network.add_module('log_std', torch.nn.Linear(128, actionSize));
    } else {
      // For discrete actions, output action probabilities
      this.network.add_module('output', torch.nn.Linear(128, actionSize));
      this.network.add_module('softmax', torch.nn.Softmax(dim: -1));
    }

    this.network.to(this.device);

    console.log('[ActorNetwork] Initialized');
    console.log(`  Action space: ${continuous ? 'continuous' : 'discrete'}`);
    console.log(`  Action size: ${actionSize}`);
  }

  forward(state: any): any {
    let stateTensor = state;
    if (!torch.is_tensor(state)) {
      stateTensor = torch.FloatTensor(state);
    }

    stateTensor = stateTensor.to(this.device);

    if (stateTensor.dim() > 2) {
      stateTensor = stateTensor.flatten(start_dim: 1);
    }

    const features = this.network[':3'](stateTensor); // First 3 layers

    if (this.continuous) {
      const mean = this.network.mean(features);
      const logStd = this.network.log_std(features);
      return { mean, logStd };
    } else {
      const output = this.network.output(features);
      const probs = this.network.softmax(output);
      return probs;
    }
  }

  getParameters(): any {
    return this.network.parameters();
  }

  getStateDict(): any {
    return this.network.state_dict();
  }

  loadStateDict(stateDict: any): void {
    this.network.load_state_dict(stateDict);
  }

  to(device: any): void {
    this.device = device;
    this.network.to(device);
  }
}

// ============================================================================
// Critic Network (Value Function)
// ============================================================================

export class CriticNetwork {
  private network: any;
  private device: any;

  constructor(stateShape: number[], device = 'cpu') {
    this.device = torch.device(device);

    const inputSize = stateShape.reduce((a, b) => a * b, 1);

    // Build value network
    this.network = torch.nn.Sequential(
      torch.nn.Linear(inputSize, 256),
      torch.nn.ReLU(),
      torch.nn.Linear(256, 256),
      torch.nn.ReLU(),
      torch.nn.Linear(256, 128),
      torch.nn.ReLU(),
      torch.nn.Linear(128, 1)
    );

    this.network.to(this.device);

    console.log('[CriticNetwork] Initialized');
  }

  forward(state: any): any {
    let stateTensor = state;
    if (!torch.is_tensor(state)) {
      stateTensor = torch.FloatTensor(state);
    }

    stateTensor = stateTensor.to(this.device);

    if (stateTensor.dim() > 2) {
      stateTensor = stateTensor.flatten(start_dim: 1);
    }

    return this.network(stateTensor).squeeze(-1);
  }

  getParameters(): any {
    return this.network.parameters();
  }

  getStateDict(): any {
    return this.network.state_dict();
  }

  loadStateDict(stateDict: any): void {
    this.network.load_state_dict(stateDict);
  }

  to(device: any): void {
    this.device = device;
    this.network.to(device);
  }
}

// ============================================================================
// PPO Agent
// ============================================================================

export class PPOAgent {
  private config: Required<PPOConfig>;
  private actor: ActorNetwork;
  private critic: CriticNetwork;
  private actorOptimizer: any;
  private criticOptimizer: any;
  private memory: PPOMemory;
  private device: any;
  private updateCounter = 0;

  constructor(config: PPOConfig) {
    // Set defaults
    this.config = {
      continuous: false,
      learningRateActor: 0.0003,
      learningRateCritic: 0.001,
      discount: 0.99,
      gaeÎ»: 0.95,
      clipRatio: 0.2,
      valueClip: 0.2,
      entropyCoef: 0.01,
      valueCoef: 0.5,
      maxGradNorm: 0.5,
      updateEpochs: 10,
      batchSize: 64,
      trajectorySize: 2048,
      device: 'cpu',
      ...config,
    };

    this.device = torch.device(this.config.device);

    // Initialize actor and critic
    this.actor = new ActorNetwork(
      this.config.stateShape,
      this.config.actionSize,
      this.config.continuous,
      this.config.device
    );

    this.critic = new CriticNetwork(
      this.config.stateShape,
      this.config.device
    );

    // Initialize optimizers
    this.actorOptimizer = torch.optim.Adam(
      this.actor.getParameters(),
      lr: this.config.learningRateActor
    );

    this.criticOptimizer = torch.optim.Adam(
      this.critic.getParameters(),
      lr: this.config.learningRateCritic
    );

    // Initialize memory
    this.resetMemory();

    console.log('\n[PPOAgent] Initialized');
    console.log(`  Clip ratio: ${this.config.clipRatio}`);
    console.log(`  GAE Î»: ${this.config.gaeÎ»}`);
    console.log(`  Entropy coefficient: ${this.config.entropyCoef}`);
    console.log(`  Update epochs: ${this.config.updateEpochs}`);
    console.log(`  Trajectory size: ${this.config.trajectorySize}`);
  }

  /**
   * Select action using current policy
   */
  selectAction(state: any, training = true): {
    action: number | number[];
    logProb: number;
    value: number;
  } {
    const stateTensor = this.preprocessState(state);

    let action: any;
    let logProb: number;
    let value: number;

    torch.no_grad(() => {
      // Get action distribution from actor
      if (this.config.continuous) {
        const { mean, logStd } = this.actor.forward(stateTensor);
        const std = logStd.exp();

        // Sample from normal distribution
        const normal = torch.distributions.Normal(mean, std);
        action = training ? normal.sample() : mean;
        logProb = normal.log_prob(action).sum(dim: -1);
      } else {
        const probs = this.actor.forward(stateTensor);

        // Sample from categorical distribution
        const categorical = torch.distributions.Categorical(probs);
        action = training ? categorical.sample() : probs.argmax();
        logProb = categorical.log_prob(action);
      }

      // Get value estimate
      value = this.critic.forward(stateTensor);
    });

    // Convert to JavaScript types
    const actionValue = this.config.continuous
      ? Array.from(action.cpu().numpy())
      : Number(action.item());

    const logProbValue = Number(logProb.item());
    const valueValue = Number(value.item());

    // Store in memory if training
    if (training) {
      this.memory.states.push(state);
      this.memory.actions.push(actionValue);
      this.memory.logProbs.push(logProbValue);
      this.memory.values.push(valueValue);
    }

    return {
      action: actionValue,
      logProb: logProbValue,
      value: valueValue,
    };
  }

  /**
   * Store reward and done flag
   */
  storeReward(reward: number, done: boolean): void {
    this.memory.rewards.push(reward);
    this.memory.dones.push(done);
  }

  /**
   * Check if ready to update
   */
  isReadyToUpdate(): boolean {
    return this.memory.states.length >= this.config.trajectorySize;
  }

  /**
   * Update policy using PPO algorithm
   */
  update(): PPOMetrics {
    if (!this.isReadyToUpdate()) {
      throw new Error('Not enough trajectory data for update');
    }

    // Calculate advantages and returns using GAE
    const { advantages, returns } = this.calculateGAE();

    // Prepare batch data
    const states = torch.FloatTensor(numpy.array(this.memory.states)).to(this.device);
    const actions = this.config.continuous
      ? torch.FloatTensor(numpy.array(this.memory.actions)).to(this.device)
      : torch.LongTensor(this.memory.actions).to(this.device);
    const oldLogProbs = torch.FloatTensor(this.memory.logProbs).to(this.device);
    const advantagesTensor = torch.FloatTensor(advantages).to(this.device);
    const returnsTensor = torch.FloatTensor(returns).to(this.device);
    const oldValues = torch.FloatTensor(this.memory.values).to(this.device);

    // Normalize advantages
    const advantagesNormalized = (advantagesTensor - advantagesTensor.mean()) /
                                  (advantagesTensor.std() + 1e-8);

    let totalPolicyLoss = 0;
    let totalValueLoss = 0;
    let totalEntropy = 0;
    let totalKL = 0;
    let totalClipFraction = 0;
    let numBatches = 0;

    // Multiple epochs of updates
    for (let epoch = 0; epoch < this.config.updateEpochs; epoch++) {
      // Create minibatches
      const indices = numpy.arange(states.shape[0]);
      numpy.random.shuffle(indices);

      for (let start = 0; start < indices.length; start += this.config.batchSize) {
        const end = Math.min(start + this.config.batchSize, indices.length);
        const batchIndices = indices.slice(start, end);

        // Get batch
        const batchStates = states[batchIndices];
        const batchActions = actions[batchIndices];
        const batchOldLogProbs = oldLogProbs[batchIndices];
        const batchAdvantages = advantagesNormalized[batchIndices];
        const batchReturns = returnsTensor[batchIndices];
        const batchOldValues = oldValues[batchIndices];

        // Evaluate actions under current policy
        let newLogProbs: any;
        let entropy: any;

        if (this.config.continuous) {
          const { mean, logStd } = this.actor.forward(batchStates);
          const std = logStd.exp();
          const normal = torch.distributions.Normal(mean, std);
          newLogProbs = normal.log_prob(batchActions).sum(dim: -1);
          entropy = normal.entropy().sum(dim: -1).mean();
        } else {
          const probs = this.actor.forward(batchStates);
          const categorical = torch.distributions.Categorical(probs);
          newLogProbs = categorical.log_prob(batchActions);
          entropy = categorical.entropy().mean();
        }

        // Calculate ratio and clipped surrogate
        const ratio = (newLogProbs - batchOldLogProbs).exp();
        const surr1 = ratio * batchAdvantages;
        const surr2 = torch.clamp(
          ratio,
          1.0 - this.config.clipRatio,
          1.0 + this.config.clipRatio
        ) * batchAdvantages;

        // Policy loss (negative because we want to maximize)
        const policyLoss = -torch.min(surr1, surr2).mean();

        // Value function loss
        const newValues = this.critic.forward(batchStates);

        if (this.config.valueClip > 0) {
          // Clipped value loss
          const valueLossUnclipped = torch.nn.functional.mse_loss(
            newValues,
            batchReturns,
            reduction: 'none'
          );

          const valuesClipped = batchOldValues + torch.clamp(
            newValues - batchOldValues,
            -this.config.valueClip,
            this.config.valueClip
          );

          const valueLossClipped = torch.nn.functional.mse_loss(
            valuesClipped,
            batchReturns,
            reduction: 'none'
          );

          const valueLoss = torch.max(valueLossUnclipped, valueLossClipped).mean();
        } else {
          const valueLoss = torch.nn.functional.mse_loss(newValues, batchReturns);
        }

        // Total loss
        const loss = (
          policyLoss +
          this.config.valueCoef * valueLoss -
          this.config.entropyCoef * entropy
        );

        // Update networks
        this.actorOptimizer.zero_grad();
        this.criticOptimizer.zero_grad();
        loss.backward();

        // Gradient clipping
        torch.nn.utils.clip_grad_norm_(
          this.actor.getParameters(),
          this.config.maxGradNorm
        );
        torch.nn.utils.clip_grad_norm_(
          this.critic.getParameters(),
          this.config.maxGradNorm
        );

        this.actorOptimizer.step();
        this.criticOptimizer.step();

        // Calculate metrics
        const kl = (batchOldLogProbs - newLogProbs).mean();
        const clipFraction = (ratio - 1.0).abs().gt(this.config.clipRatio).float().mean();

        totalPolicyLoss += Number(policyLoss.item());
        totalValueLoss += Number(valueLoss.item());
        totalEntropy += Number(entropy.item());
        totalKL += Number(kl.item());
        totalClipFraction += Number(clipFraction.item());
        numBatches++;
      }
    }

    // Calculate explained variance
    const explainedVar = this.explainedVariance(
      oldValues.cpu().numpy(),
      returnsTensor.cpu().numpy()
    );

    // Reset memory
    this.resetMemory();
    this.updateCounter++;

    return {
      policyLoss: totalPolicyLoss / numBatches,
      valueLoss: totalValueLoss / numBatches,
      entropy: totalEntropy / numBatches,
      totalLoss: (totalPolicyLoss + totalValueLoss) / numBatches,
      kl: totalKL / numBatches,
      clipFraction: totalClipFraction / numBatches,
      explainedVariance: explainedVar,
    };
  }

  /**
   * Calculate Generalized Advantage Estimation (GAE)
   */
  private calculateGAE(): { advantages: number[]; returns: number[] } {
    const advantages: number[] = [];
    const returns: number[] = [];

    let lastGAE = 0;
    const length = this.memory.rewards.length;

    // Calculate GAE backward through trajectory
    for (let t = length - 1; t >= 0; t--) {
      const isDone = this.memory.dones[t];
      const reward = this.memory.rewards[t];
      const value = this.memory.values[t];
      const nextValue = t < length - 1 ? this.memory.values[t + 1] : 0;

      // TD error: Î´ = r + Î³V(s') - V(s)
      const delta = reward + (isDone ? 0 : this.config.discount * nextValue) - value;

      // GAE: A = Î´ + Î³Î»Î´' + (Î³Î»)Â²Î´'' + ...
      lastGAE = delta + (isDone ? 0 : this.config.discount * this.config.gaeÎ» * lastGAE);

      advantages.unshift(lastGAE);
      returns.unshift(lastGAE + value);
    }

    return { advantages, returns };
  }

  /**
   * Calculate explained variance
   */
  private explainedVariance(predictions: any, targets: any): number {
    const predArray = Array.from(predictions);
    const targetArray = Array.from(targets);

    const varY = this.variance(targetArray);
    const varResidual = this.variance(
      targetArray.map((y, i) => y - predArray[i])
    );

    if (varY === 0) return 0;
    return 1 - varResidual / varY;
  }

  /**
   * Calculate variance
   */
  private variance(arr: number[]): number {
    const mean = arr.reduce((sum, val) => sum + val, 0) / arr.length;
    const squaredDiffs = arr.map(val => Math.pow(val - mean, 2));
    return squaredDiffs.reduce((sum, val) => sum + val, 0) / arr.length;
  }

  /**
   * Preprocess state
   */
  private preprocessState(state: any): any {
    if (torch.is_tensor(state)) {
      return state.to(this.device);
    }

    const stateTensor = torch.FloatTensor(state);
    return stateTensor.unsqueeze(0).to(this.device);
  }

  /**
   * Reset memory
   */
  private resetMemory(): void {
    this.memory = {
      states: [],
      actions: [],
      logProbs: [],
      rewards: [],
      values: [],
      dones: [],
    };
  }

  /**
   * Get number of updates performed
   */
  getUpdateCount(): number {
    return this.updateCounter;
  }

  /**
   * Save model
   */
  save(filepath: string): void {
    const checkpoint = {
      actor: this.actor.getStateDict(),
      critic: this.critic.getStateDict(),
      actorOptimizer: this.actorOptimizer.state_dict(),
      criticOptimizer: this.criticOptimizer.state_dict(),
      updateCounter: this.updateCounter,
      config: this.config,
    };

    torch.save(checkpoint, filepath);
    console.log(`[PPO] Model saved to ${filepath}`);
  }

  /**
   * Load model
   */
  load(filepath: string): void {
    const checkpoint = torch.load(filepath);

    this.actor.loadStateDict(checkpoint.actor);
    this.critic.loadStateDict(checkpoint.critic);
    this.actorOptimizer.load_state_dict(checkpoint.actorOptimizer);
    this.criticOptimizer.load_state_dict(checkpoint.criticOptimizer);
    this.updateCounter = checkpoint.updateCounter;

    console.log(`[PPO] Model loaded from ${filepath}`);
    console.log(`  Updates: ${this.updateCounter}`);
  }

  /**
   * Evaluate state value
   */
  evaluateValue(state: any): number {
    const stateTensor = this.preprocessState(state);
    const value = this.critic.forward(stateTensor);
    return Number(value.item());
  }

  /**
   * Get action probabilities (discrete actions only)
   */
  getActionProbabilities(state: any): number[] {
    if (this.config.continuous) {
      throw new Error('Action probabilities only available for discrete actions');
    }

    const stateTensor = this.preprocessState(state);
    const probs = this.actor.forward(stateTensor);
    return Array.from(probs.squeeze().detach().cpu().numpy());
  }
}

// ============================================================================
// PPO with Recurrent Networks (PPO-LSTM)
// ============================================================================

export class RecurrentPPOAgent extends PPOAgent {
  private hiddenSize: number;
  private sequenceLength: number;
  private hiddenStates: any;

  constructor(config: PPOConfig & { hiddenSize?: number; sequenceLength?: number }) {
    super(config);
    this.hiddenSize = config.hiddenSize || 256;
    this.sequenceLength = config.sequenceLength || 16;

    console.log('[RecurrentPPO] Using LSTM with:');
    console.log(`  Hidden size: ${this.hiddenSize}`);
    console.log(`  Sequence length: ${this.sequenceLength}`);
  }

  /**
   * Reset hidden states
   */
  resetHidden(): void {
    this.hiddenStates = null;
  }

  /**
   * Select action with recurrent policy
   */
  override selectAction(state: any, training = true): any {
    // Implementation would use LSTM layers
    // For now, delegate to parent
    return super.selectAction(state, training);
  }
}

// ============================================================================
// Utility Functions
// ============================================================================

/**
 * Calculate discounted returns
 */
export function calculateReturns(
  rewards: number[],
  dones: boolean[],
  discount: number,
  normalize = true
): number[] {
  const returns: number[] = [];
  let runningReturn = 0;

  for (let t = rewards.length - 1; t >= 0; t--) {
    runningReturn = rewards[t] + (dones[t] ? 0 : discount * runningReturn);
    returns.unshift(runningReturn);
  }

  if (normalize) {
    const mean = returns.reduce((sum, r) => sum + r, 0) / returns.length;
    const std = Math.sqrt(
      returns.reduce((sum, r) => sum + Math.pow(r - mean, 2), 0) / returns.length
    );
    return returns.map(r => (r - mean) / (std + 1e-8));
  }

  return returns;
}

/**
 * Calculate advantages from values and rewards
 */
export function calculateAdvantages(
  rewards: number[],
  values: number[],
  dones: boolean[],
  discount: number,
  gaeÎ»: number
): number[] {
  const advantages: number[] = [];
  let lastGAE = 0;

  for (let t = rewards.length - 1; t >= 0; t--) {
    const nextValue = t < rewards.length - 1 ? values[t + 1] : 0;
    const delta = rewards[t] + (dones[t] ? 0 : discount * nextValue) - values[t];
    lastGAE = delta + (dones[t] ? 0 : discount * gaeÎ» * lastGAE);
    advantages.unshift(lastGAE);
  }

  return advantages;
}

/**
 * Compute policy gradient loss
 */
export function policyGradientLoss(
  newLogProbs: any,
  oldLogProbs: any,
  advantages: any,
  clipRatio: number
): any {
  const ratio = (newLogProbs - oldLogProbs).exp();
  const surr1 = ratio * advantages;
  const surr2 = torch.clamp(ratio, 1.0 - clipRatio, 1.0 + clipRatio) * advantages;
  return -torch.min(surr1, surr2).mean();
}

/**
 * Compute KL divergence between old and new policies
 */
export function computeKL(oldLogProbs: any, newLogProbs: any): number {
  const kl = (oldLogProbs - newLogProbs).mean();
  return Number(kl.item());
}

// ============================================================================
// Example Usage
// ============================================================================

if (import.meta.main) {
  console.log('ðŸŽ® Proximal Policy Optimization (PPO) Implementation\n');
  console.log('This demonstrates:');
  console.log('  - Actor-Critic architecture with PyTorch');
  console.log('  - Generalized Advantage Estimation (GAE)');
  console.log('  - Clipped surrogate objective');
  console.log('  - All in TypeScript via Elide!\n');

  // Create PPO agent
  const agent = new PPOAgent({
    stateShape: [4],
    actionSize: 2,
    continuous: false,
    learningRateActor: 0.0003,
    learningRateCritic: 0.001,
    discount: 0.99,
    gaeÎ»: 0.95,
    clipRatio: 0.2,
    entropyCoef: 0.01,
    updateEpochs: 10,
    batchSize: 64,
    trajectorySize: 2048,
    device: 'cpu',
  });

  console.log('âœ… PPO Agent created successfully!');
  console.log('\nAgent features:');
  console.log('  - Separate actor and critic networks');
  console.log('  - GAE for advantage estimation');
  console.log('  - Multiple update epochs per trajectory');
  console.log('  - Entropy regularization');
  console.log('  - Gradient clipping');
  console.log('  - Value function clipping');
}
