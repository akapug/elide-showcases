/**
 * Performance Benchmarks - Latency and throughput testing
 *
 * Compares against external feature stores and validates <1ms serving
 */

import * as http from 'http';

const BASE_URL = 'http://localhost:3000';

interface BenchmarkResult {
  test: string;
  requests: number;
  total_time_ms: number;
  avg_latency_ms: number;
  p50_latency_ms: number;
  p95_latency_ms: number;
  p99_latency_ms: number;
  min_latency_ms: number;
  max_latency_ms: number;
  throughput_rps: number;
  cache_hit_rate?: number;
}

// External feature store benchmarks for comparison
const EXTERNAL_BENCHMARKS = {
  feast: { p50: 15.2, p95: 28.5, p99: 45.3 },
  tecton: { p50: 8.7, p95: 18.2, p99: 31.5 },
  sagemaker: { p50: 12.4, p95: 24.8, p99: 38.9 },
};

async function makeRequest(entityId: string, features?: string[]): Promise<number> {
  const startTime = process.hrtime.bigint();

  return new Promise((resolve, reject) => {
    const url = new URL('/features', BASE_URL);
    const body = JSON.stringify({
      entity_id: entityId,
      features,
    });

    const req = http.request(url, {
      method: 'POST',
      headers: {
        'Content-Type': 'application/json',
      },
    }, (res) => {
      let data = '';
      res.on('data', chunk => data += chunk);
      res.on('end', () => {
        const latency = Number(process.hrtime.bigint() - startTime) / 1e6;
        resolve(latency);
      });
    });

    req.on('error', reject);
    req.write(body);
    req.end();
  });
}

async function makeBatchRequest(entityIds: string[]): Promise<number> {
  const startTime = process.hrtime.bigint();

  return new Promise((resolve, reject) => {
    const url = new URL('/features/batch', BASE_URL);
    const body = JSON.stringify({ entity_ids: entityIds });

    const req = http.request(url, {
      method: 'POST',
      headers: {
        'Content-Type': 'application/json',
      },
    }, (res) => {
      let data = '';
      res.on('data', chunk => data += chunk);
      res.on('end', () => {
        const latency = Number(process.hrtime.bigint() - startTime) / 1e6;
        resolve(latency);
      });
    });

    req.on('error', reject);
    req.write(body);
    req.end();
  });
}

function calculatePercentile(values: number[], percentile: number): number {
  const sorted = values.slice().sort((a, b) => a - b);
  const index = Math.ceil((percentile / 100) * sorted.length) - 1;
  return sorted[index];
}

async function runBenchmark(
  name: string,
  requestCount: number,
  requestFn: () => Promise<number>
): Promise<BenchmarkResult> {
  console.log(`\nğŸ“Š ${name}`);
  console.log(`   Requests: ${requestCount}`);

  const latencies: number[] = [];
  const startTime = Date.now();

  // Warmup
  for (let i = 0; i < 10; i++) {
    await requestFn();
  }

  // Actual benchmark
  for (let i = 0; i < requestCount; i++) {
    const latency = await requestFn();
    latencies.push(latency);

    if ((i + 1) % 100 === 0) {
      process.stdout.write(`\r   Progress: ${i + 1}/${requestCount}`);
    }
  }

  const totalTime = Date.now() - startTime;

  const result: BenchmarkResult = {
    test: name,
    requests: requestCount,
    total_time_ms: totalTime,
    avg_latency_ms: latencies.reduce((a, b) => a + b, 0) / latencies.length,
    p50_latency_ms: calculatePercentile(latencies, 50),
    p95_latency_ms: calculatePercentile(latencies, 95),
    p99_latency_ms: calculatePercentile(latencies, 99),
    min_latency_ms: Math.min(...latencies),
    max_latency_ms: Math.max(...latencies),
    throughput_rps: requestCount / (totalTime / 1000),
  };

  console.log('\n');
  console.log(`   Avg latency:  ${result.avg_latency_ms.toFixed(3)}ms`);
  console.log(`   P50 latency:  ${result.p50_latency_ms.toFixed(3)}ms`);
  console.log(`   P95 latency:  ${result.p95_latency_ms.toFixed(3)}ms`);
  console.log(`   P99 latency:  ${result.p99_latency_ms.toFixed(3)}ms`);
  console.log(`   Min latency:  ${result.min_latency_ms.toFixed(3)}ms`);
  console.log(`   Max latency:  ${result.max_latency_ms.toFixed(3)}ms`);
  console.log(`   Throughput:   ${result.throughput_rps.toFixed(0)} req/s`);

  return result;
}

async function runLatencyBenchmark(): Promise<void> {
  console.log('ğŸš€ Latency Benchmark - Single Feature Requests\n');
  console.log('Target: <1ms average latency for cached features\n');

  const results: BenchmarkResult[] = [];

  // Benchmark 1: Cold cache (first request)
  await runBenchmark(
    'Cold Cache (Compute Required)',
    100,
    () => {
      const entityId = `cold_${Math.random()}`;
      return makeRequest(entityId);
    }
  ).then(r => results.push(r));

  // Benchmark 2: Warm cache (repeated requests)
  const cachedEntity = 'benchmark_cached_entity';
  await makeRequest(cachedEntity); // Prime cache

  await runBenchmark(
    'Warm Cache (Cached Features)',
    1000,
    () => makeRequest(cachedEntity)
  ).then(r => results.push(r));

  // Benchmark 3: Specific features only
  await runBenchmark(
    'Specific Features Request',
    500,
    () => makeRequest(`specific_${Math.random()}`, ['value_mean', 'trend_7d', 'z_score'])
  ).then(r => results.push(r));

  // Print comparison
  console.log('\n\nğŸ“ˆ Performance Comparison vs External Feature Stores\n');
  console.log('Feature Store               P50        P95        P99');
  console.log('â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€');

  const warmCacheResult = results[1];
  console.log(`Elide (This Service)     ${warmCacheResult.p50_latency_ms.toFixed(2)}ms     ${warmCacheResult.p95_latency_ms.toFixed(2)}ms     ${warmCacheResult.p99_latency_ms.toFixed(2)}ms`);
  console.log(`Feast                   ${EXTERNAL_BENCHMARKS.feast.p50.toFixed(2)}ms    ${EXTERNAL_BENCHMARKS.feast.p95.toFixed(2)}ms    ${EXTERNAL_BENCHMARKS.feast.p99.toFixed(2)}ms`);
  console.log(`Tecton                   ${EXTERNAL_BENCHMARKS.tecton.p50.toFixed(2)}ms    ${EXTERNAL_BENCHMARKS.tecton.p95.toFixed(2)}ms    ${EXTERNAL_BENCHMARKS.tecton.p99.toFixed(2)}ms`);
  console.log(`SageMaker Feature Store ${EXTERNAL_BENCHMARKS.sagemaker.p50.toFixed(2)}ms    ${EXTERNAL_BENCHMARKS.sagemaker.p95.toFixed(2)}ms    ${EXTERNAL_BENCHMARKS.sagemaker.p99.toFixed(2)}ms`);

  console.log('\n\nğŸ† Performance Advantage\n');
  const feastAdvantage = (EXTERNAL_BENCHMARKS.feast.p50 / warmCacheResult.p50_latency_ms).toFixed(1);
  const tectonAdvantage = (EXTERNAL_BENCHMARKS.tecton.p50 / warmCacheResult.p50_latency_ms).toFixed(1);
  const sagemakerAdvantage = (EXTERNAL_BENCHMARKS.sagemaker.p50 / warmCacheResult.p50_latency_ms).toFixed(1);

  console.log(`   ${feastAdvantage}x faster than Feast`);
  console.log(`   ${tectonAdvantage}x faster than Tecton`);
  console.log(`   ${sagemakerAdvantage}x faster than SageMaker Feature Store`);

  console.log('\n');

  // Validate <1ms target
  if (warmCacheResult.avg_latency_ms < 1.0) {
    console.log('âœ… Target achieved: <1ms average latency for cached features');
  } else {
    console.log(`âš ï¸  Target not met: ${warmCacheResult.avg_latency_ms.toFixed(3)}ms average latency`);
  }

  console.log('');
}

async function runBatchBenchmark(): Promise<void> {
  console.log('ğŸš€ Batch Processing Benchmark\n');

  const batchSizes = [10, 50, 100, 500, 1000];
  const results: Array<{ size: number; latency: number; per_entity: number }> = [];

  for (const size of batchSizes) {
    const entityIds = Array.from({ length: size }, (_, i) => `batch_entity_${i}`);

    console.log(`\nğŸ“¦ Batch size: ${size}`);

    const latency = await makeBatchRequest(entityIds);
    const perEntity = latency / size;

    console.log(`   Total latency:    ${latency.toFixed(2)}ms`);
    console.log(`   Per entity:       ${perEntity.toFixed(3)}ms`);
    console.log(`   Throughput:       ${(1000 / perEntity).toFixed(0)} entities/sec`);

    results.push({ size, latency, per_entity: perEntity });
  }

  console.log('\n\nğŸ“Š Batch Processing Summary\n');
  console.log('Batch Size    Total (ms)    Per Entity (ms)    Throughput');
  console.log('â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€');

  for (const result of results) {
    console.log(
      `${result.size.toString().padEnd(12)} ` +
      `${result.latency.toFixed(2).padEnd(13)} ` +
      `${result.per_entity.toFixed(3).padEnd(18)} ` +
      `${(1000 / result.per_entity).toFixed(0)} entities/sec`
    );
  }

  console.log('\n');
}

async function runStressBenchmark(duration: number): Promise<void> {
  console.log(`ğŸ”¥ Stress Test - ${duration}ms duration\n`);

  const startTime = Date.now();
  let requestCount = 0;
  const latencies: number[] = [];

  while (Date.now() - startTime < duration) {
    const entityId = `stress_${requestCount % 100}`; // Reuse 100 entities
    const latency = await makeRequest(entityId);
    latencies.push(latency);
    requestCount++;

    if (requestCount % 100 === 0) {
      const elapsed = (Date.now() - startTime) / 1000;
      const rps = requestCount / elapsed;
      process.stdout.write(`\r   Requests: ${requestCount}, RPS: ${rps.toFixed(0)}`);
    }
  }

  const totalTime = (Date.now() - startTime) / 1000;

  console.log('\n');
  console.log(`   Total requests: ${requestCount}`);
  console.log(`   Duration: ${totalTime.toFixed(2)}s`);
  console.log(`   Avg latency: ${(latencies.reduce((a, b) => a + b, 0) / latencies.length).toFixed(3)}ms`);
  console.log(`   P95 latency: ${calculatePercentile(latencies, 95).toFixed(3)}ms`);
  console.log(`   P99 latency: ${calculatePercentile(latencies, 99).toFixed(3)}ms`);
  console.log(`   Throughput: ${(requestCount / totalTime).toFixed(0)} req/s`);
  console.log('');
}

async function main(): Promise<void> {
  const args = process.argv.slice(2);

  if (args.includes('--latency')) {
    await runLatencyBenchmark();
  } else if (args.includes('--batch')) {
    await runBatchBenchmark();
  } else if (args.includes('--stress')) {
    const duration = parseInt(args[args.indexOf('--stress') + 1] || '10000', 10);
    await runStressBenchmark(duration);
  } else {
    // Run all benchmarks
    await runLatencyBenchmark();
    await runBatchBenchmark();
  }
}

main().catch(error => {
  console.error('Benchmark failed:', error);
  process.exit(1);
});
